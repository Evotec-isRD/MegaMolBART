defaults:
  - trainer: trainer_defaults
  - model/dataset@model.train_ds: train_ds_defaults
  - model/dataset@model.validation_ds: validation_ds_defaults
  - exp_manager: log_all
  - model: xsmall
  - model/optim@model.optim: adam
  - model/optim/sched@model.optim.sched: transformer

name: &name MegaMolBART
do_training: true # set to false if data preprocessing steps must be completed
do_testing: false # set to true to run evaluation on test data after training, requires test_ds section
seed: 42
dataset_path: /data/zinc_csv/

exp_manager:
  name: *name
  wandb_logger_kwargs:
    project: *name
    group: ${model.name}
    job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.gpus}
    name: ${model.name}_${now:%y%m%d-%H%M%S}
    entity: clara-discovery
    notes: "date: ${now:%y%m%d-%H%M%S}"
    tags:
      - *name
      - ${model.name}

trainer:
  gpus: 8
  num_nodes: 1
  min_epochs: 10
  max_epochs: 10
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  num_sanity_val_steps: 2 # set to 0 or small number to test validation before completing a full epoch
  val_check_interval: 0.5 # fractions indicate epoch fractions, integers indicate iterations can also use check_val_every_n_epoc
  # auto_scale_batch_size: None/binsearch

tokenizer:
  vocab_path: /workspace/nemo/nemo/collections/chem/vocab/megamolbart_pretrain_vocab.txt
  mask_prob: 0.15

model:
  name: pretrain_xsmall_span_aug
  train_ds:
    encoder_mask: true # task = mask(span)_aug
    decoder_augment: true
    filepath: ${dataset_path}/train/x[000..146].csv
    metadata_path: ${dataset_path}/train/metadata.txt
    micro_batch_size: 32
    num_workers: 8

  validation_ds:
    encoder_mask: false # not done in validation set
    decoder_augment: false
    filepath: ${dataset_path}/val/x[000..146].csv
    metadata_path: ${dataset_path}/val/metadata.txt
    micro_batch_size: 32
    num_workers: 8
