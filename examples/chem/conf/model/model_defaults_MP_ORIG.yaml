seq_len: 512
max_position_embeddings: ${.seq_len}
encoder_arch: 'transformer'
decoder_arch: 'transformer'
hidden_dropout: 0.1 # Dropout probability for hidden state transformer.
kv_channels: null # Projection weights dimension in multi-head attention. Set to hidden_size // num_attention_heads if null
apply_query_key_layer_scaling: False # scale Q * K^T by 1 / layer-number.
persist_layer_norm: True # Use of persistent fused layer norm kernel.
layernorm_epsilon: 1e-5
init_method_std: 0.02 # Standard deviation of the zero mean normal distribution used for weight initialization.')
gradient_as_bucket_view: True # Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
# pretrained: False # TODO ADD BACK
# checkpoint_file: null # /path/to/megatron/checkpoint/model_optim_rng.pt
# encoder_type: seq2seq
# blocks_model: null
# steps_model: null
 
