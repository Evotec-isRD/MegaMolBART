encoder_arch: 'transformer'
decoder_arch: 'transformer'
activation: 'gelu'
init_method_std: 0.02 # standard deviation of the zero mean normal distribution used for weight initialization.')
hidden_dropout: 0.1 # dropout probability for hidden state transformer.
attention_dropout: 0.1 # dropout probability in the attention layer.
kv_channels: null # projection weights dimension in multi-head attention. Set to hidden_size // num_attention_heads if null
apply_query_key_layer_scaling: True # scale Q * K^T by 1 / layer-number.
layernorm_epsilon: 1e-5
pre_process: True # add embedding
post_process: True # add pooler  

# memory efficiency
persist_layer_norm: True # use persistent fused layer norm kernel.
gradient_as_bucket_view: True # allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
make_vocab_size_divisible_by: 128 # pad the vocab size to be divisible by this value for computation efficiency. Default: 128
masked_softmax_fusion: True # batch padding size must be divisible by 8 if True, otherwise will result in crash due to CUDA alignment issue

# precision
native_amp_init_scale: 4294967296 # 2 ** 32
native_amp_growth_interval: 1000
megatron_amp_O2: False # use AMP with O2 style mixed precision instead of native amp on-the-fly weight autocasting.
fp32_residual_connection: False # move residual connections to fp32
fp16_lm_cross_entropy: False # move the cross entropy unreduced loss calculation for lm head to fp16

# miscellaneous
seed: ${seed}
use_cpu_initialization: False # init weights on the CPU (slow for large models)
onnx_safe: False # use work-arounds for known problems with Torch ONNX exporter

# not implemented in NeMo yet
activations_checkpoint_method: null # 'uniform', 'block'
activations_checkpoint_num_layers: 1
